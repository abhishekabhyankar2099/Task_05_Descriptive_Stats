# Task_05_Descriptive_Stats

## Overview

This project explores the capabilities of Large Language Models (LLMs) to analyze and answer natural language questions on a structured NBA player statistics dataset. Using Syracuse University Women’s Lacrosse data as a model for dataset complexity, the project aims to compare answers generated by an LLM (such as Perplexity or ChatGPT) against ground-truth calculations performed with Python.

---

## Dataset

The dataset consists of NBA player statistics from a recent season, including metrics such as player names, teams, ages, games played, minutes played, points scored, free throw percentage, and awards.

**Note:** The dataset file itself is not included in this repository, as per assignment guidelines. It was sourced from a publicly available database.

---

## Research Questions

The following key questions were posed to both the LLM and analyzed via Python:

1. Who has the most points per match?
2. Who has the most awards?
3. Who has the greatest free throw percentage?
4. What is the average age of each team?
5. Which player has played the most minutes in total?

---

## Methodology

- The dataset was analyzed using Python and pandas to compute exact answers to the above questions.
- Simultaneously, the same questions were posed to an LLM (Perplexity or ChatGPT) using natural language prompts.
- The LLM responses were then compared to the Python results to evaluate accuracy and reasoning.
- Prompt engineering was applied to ensure clarity in questions posed to the LLM.
- Observations on discrepancies and LLM limitations were documented.

---

## Results

**Python-derived answers:**

- Most points per match: Joel Embiid — 34.69 points/match
- Most awards: Shai Gilgeous-Alexander — 5 awards (MVP-2, DPOY-7, CPOY-3, AS, NBA1)
- Greatest free throw percentage: David Roddy — 100.00%
- Average age by team: See full list in analysis script
- Most minutes played: DeMar DeRozan — 2989 minutes

**LLM answers** were consistent with the Python results for all above questions when given clear prompts and data context.

---

## Lessons Learned

- LLMs can accurately answer straightforward, quantitative questions if provided with structured data and clear instructions.
- Validation using script-based calculations is critical to verify LLM output.
- Prompt clarity and specificity significantly impact LLM performance.
- For complex or subjective queries, LLMs may require explicit definitions to avoid misinterpretations.

---

## How to Run the Python Analysis

1. Ensure Python 3.x is installed.
2. Install dependencies:  
